---
title: "On the Representational Capacity of Recurrent Neural Language Models"
collection: publications
permalink: /publication/failure-arcs-backward
excerpt: 'This work investigates the computational expressivity of language models based on recurrent neural networks. We extend the Turing completeness result by Siegelmann and Sontag (1992) to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM).'
date: 2023-12-06
venue: 'EMNLP 2023'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
# citation: 'Nowak, F., Svete, A., Du, L., & Cotterell, R. (2023, December). On the Representational Capacity of Recurrent Neural Language Models. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore, Singapore: Association for Computational Linguistics.'
---
This work investigates the computational expressivity of language models based on recurrent neural networks. We extend the Turing completeness result by Siegelmann and Sontag (1992) to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM).

[Download the paper here](http://anejsvete.github.io/files/rnn-lms.pdf)

Citation `BibTeX`:
``` bibtex
@inproceedings{nowak-etal-2023-representational,
    title = "On the Representational Capacity of Recurrent Neural Language Models",
    author = "Nowak, Franz  and
      Svete, Anej  and
      Du, Li  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore, Singapore",
    publisher = "Association for Computational Linguistics",
}
```